# 调参
在使用 XGBoost 进行模型训练时，可以调整多个超参数来优化模型性能。以下是一些常见的参数及其建议调整范围：

## max_depth
作用: 控制树的最大深度，影响模型的复杂度。
建议值: [3, 4, 5, 6, 7, 8, 9, 10]
注意: 较大的深度会增加模型的复杂度，可能导致过拟合。
## eta (学习率)
作用: 控制每棵树的权重更新幅度，影响模型的学习速度。
建议值: [0.01, 0.05, 0.1, 0.2]
注意: 较小的学习率通常需要更多的 boosting 轮数。
## num_boost_round (提升轮数)
作用: 控制模型训练的迭代次数。
建议值: [50, 100, 200, 300]
注意: 可与 early_stopping_rounds 一起使用，以防止过拟合。
## subsample (子样本比例)
作用: 控制用于训练每棵树的样本比例，可以减小过拟合的风险。
建议值: [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
注意: 较小的比例可能导致欠拟合。
## colsample_bytree (列采样比例)
作用: 控制构建每棵树时随机选择的特征比例。
建议值: [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
注意: 有助于降低过拟合。
## gamma (最小损失减小量)
作用: 控制是否进行节点分裂的权重。值越大，越难分裂。
建议值: [0, 0.1, 0.2, 0.3, 0.4, 0.5]
注意: 增加 gamma 会减少树的复杂度。
## lambda (L2 正则化)
作用: 控制 L2 正则化强度，防止过拟合。
建议值: [0, 1, 5, 10]
注意: 增加正则化强度可以降低模型复杂度。
## alpha (L1 正则化)
作用: 控制 L1 正则化强度，防止过拟合。
建议值: [0, 1, 5, 10]
注意: 与 lambda 类似，但效果不同。
示例：使用网格搜索
你可以将这些参数组合起来，通过网格搜索找到最佳参数组合。

下面是一个示例代码：

```Python

from sklearn.model_selection import GridSearchCV
import xgboost as xgb

# 创建 XGBoost 分类器
model = xgb.XGBRegressor()

# 定义参数网格
param_grid = {
    'max_depth': [3, 4, 5, 6],
    'eta': [0.01, 0.1, 0.2],
    'subsample': [0.5, 0.7, 1.0],
    'colsample_bytree': [0.5, 0.7, 1.0],
    'num_boost_round': [50, 100],
    'gamma': [0, 0.1, 0.2],
    'lambda': [0, 1, 5]
}

# 使用 GridSearchCV 进行参数搜索
grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5)
grid_search.fit(X_train, y_train)

# 输出最佳参数
print("Best parameters found: ", grid_search.best_params_)
```

# 训练结果
```
(503, 322)
(467, 322)
count       94.000000
mean      8400.683719
std       9750.697399
min         12.272204
25%       1931.062544
50%       3702.752721
75%      12245.263500
max      37248.613351
MSE: 7070635.54, RMSE: 2659.07
```
## 一条测试集中的数据
```Python
sector = "Utilities"
X = 
[nan, 335816000.0, 93782000.0, -2299294000.0, 
-75872000.0, -169654000.0, nan, 157953000.0, 
733206000.0, nan, nan, 68435000.0, 
nan, 302081000.0, 4509348000.0, 141155000.0,
nan, nan, 5636723000.0, nan]
y = 8.086851 # 此时的y是取对数的结果
```